{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author: Etienne Bonnassieux\n",
    "## Comments, questions? Contact etienne.bonnassieux@unibo.it\n",
    "\n",
    "# **I. Prerequisites**\n",
    "\n",
    "In this jupyter notebook, we will learn how to perform a round of self-calibration using killMS & DDFacet on a LOFAR dataset. All the required software will be pre-installed on the singularity you'll be using for the school, and the data you will work on will be provided alongside it. If you want to do this later on your own machine and in your own time, you must first have the following installed (non-exhaustive list, I probably forgot at least one thing):\n",
    "\n",
    "- the KERN suite http://kernsuite.info/installation/\n",
    "- ddfacet - sudo apt-get install ddfacet; pip install ephem\n",
    "- dysco (if using a LOFAR dataset) - sudo apt-get install dysco\n",
    "- killMS, which you can acquire through saopicc: https://github.com/saopicc/killMS\n",
    "\n",
    "taking care to follow the instructions in individual gits where appropriate. Note that all the installs recommended here only work after installing KERN. If you don't want to use KERN, that is fine! Just be prepared for a good week of tears and sorrow. Note that KERN also comes with a lot of other useful radio-astro tools; I personally recommend tigger as an image viewer (takes a while to get used to but has some very useful functions) and pybdsf if you wish to get deeper than this tutorial in interferometric imaging.\n",
    "\n",
    "Note that you can also get the Wirtinger suite directly from the public release repositories:\n",
    "\n",
    "* https://github.com/saopicc/DDFacet\n",
    "* https://github.com/saopicc/killMS\n",
    "\n",
    "and follow the installation instructions there. There are usually a host of problems with that sort of installation, as with all radio-astronomy software. KERN allows you to avoid them, to an extent. Note that you can find the best \"documentation\" for DDFacet (and similarly for killMS, to a much lesser degree) in their respective parset.cfg file. They can be found here:\n",
    "\n",
    "https://github.com/saopicc/DDFacet/blob/master/DDFacet/Parset/DefaultParset.cfg\n",
    "https://github.com/saopicc/killMS/blob/master/Parset/DefaultParset.cfg\n",
    "\n",
    "\n",
    "Finally, should you want a slightly more convenient install than the above, Martin Hardcastle's DDF-pipeline installs them for you:\n",
    "\n",
    "* https://github.com/mhardcastle/ddf-pipeline\n",
    "\n",
    "For the purposes of the LOFAR school, the necessary software was installed as part of a local singularity by our talented and handsome colleagues in IT; the information above is therefore included for those users who wish to use this software to reduce data on machines they are responsible for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **II. Launching the singularity**\n",
    "\n",
    "The first thing we'll need to do is to set up the singularity properly so as to be able to carry out this tutorial session. The way to do this is specific to the Bologna cluster for now, as far as I'm aware, but efforts to standardise this approach across all italian LOFAR computing infrastructures are ongoing. Enter the following commands in bash. If they are in separate cells, they can be entered in clusters but not all at once (i.e. enter all the commands for one cell first, then all commands for the next, and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lofar-setup  190516.17pub\n",
    ". /irasoft/common/irainit/irainit_setup-alias.sh\n",
    "singularity-setup\n",
    "lofar-singularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source /opt/lofar-cfg.sh\n",
    "source /opt/lofar-init.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **III. Introduction**\n",
    "\n",
    "\n",
    "The aim of this tutorial is to calibrate & image a small LOFAR dataset using the so-called \"Wirtinger Pack\": DDFacet and killMS. Once you have finished this tutorial, you should have some basic understanding of how to run these software packages to reduce & image your own datasets. Here is the order of operations we will follow:\n",
    "\n",
    "- Inspecting the dataset\n",
    "- Making a first image using DDFacet\n",
    "- Making a model from this image\n",
    "- Perform a direction-independent calibration run on our data with our model\n",
    "- Imaging the newly-calibrated data\n",
    "- Do the same with more than 1 direction\n",
    "\n",
    "These are two separate self-calibration loops: one is direction-independent, and one is direction-dependent. We'll begin by creating a DATA folder, putting one of our pointing's dataset there, and loading the singlarity. Type the following commands in your command terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir DATA\n",
    "cp /iranet/home2/lofar/botteon/LOFAR_SCHOOL_DR1/GSM_CAL_L232875_ABN_174.tar.gz DATA\n",
    "cd DATA\n",
    "tar -xvf GSM_CAL_L232875_ABN_174.tar.gz\n",
    "rm *MS/*lock\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have thus created a DATA directory where we've moved our downloaded dataset, untarred it, removed the lock, and then we returned to our working directory. It can be inspected with tools like msoverview, casaviewer, or the pyrap packages; these are generally dependent on the casacore software. Let's inspect the dataset with pyrap to see what columns we have in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrap.tables import table\n",
    "t=table(\"prefactor/results/L232875_SB070_uv.dppp.pre-cal_124B2FCD4t_134MHz.pre-cal.ms/\")\n",
    "t.colnames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have a number of columns in this dataset. In particular, we have \"DATA\" and \"CORRECTED_DATA\". This tells us that the data was calibrated already; in this case, it was calibrated with prefactor in amplitude and phase. Let's image this calibrated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IV. DD Self-cal - First step: imaging prefactored data with DDFacet**\n",
    "\n",
    "\n",
    "Here, we will explore using DDFacet to image some interferometric data contained in a Measurement Set, the standard dataset for LOFAR. Let's start by taking a little look at the options available. Run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDF.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there's quite a few!...in fact, the vast majority of them can be safely ignored. For starters, let's make a small image with our dataset. Since we're running this on LOFAR surveys data, for which the software was developed, we can safely run it with mostly default options. We will use three non-default options here: first, we specify the primary beam of our observation, as this will allow us to get images in apparenty and integrated flux (--Beam-Model LOFAR). We will also specify the number of CPUs to use explicitly, to avoid taxing the machine too much (--Parallel-NCPU 4). Finally, since there is no default for the dataset to be imaged, we specify it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " DDF.py --Data-MS prefactor/results/L232875_SB070_uv.dppp.pre-cal_124B2FCD4t_134MHz.pre-cal.4-4.5h.ms/ --Parallel-NCPU 4 --Beam-Model LOFAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all went well, you should now have a restored image of our prefactored data. Let's take a look at a few of the images created by our run here: by default, these will all have the prefix \"image\" and be in the current directory. We can inspect the fits files with ds9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds9 image*fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should open a lot of different images. The ones you are most likely to want to inspect are:\n",
    "\n",
    "- image.int.restored.fits (if you applied the LOFAR beam)\n",
    "- image.dirty.fits        (to see if anything was catastrophically wrong with the data)\n",
    "- image.int.residual.fits (to see what your CLEAN components are)\n",
    "\n",
    "in this example, because we specified a primary beam model, it was automatically applied to create the .int files (which show the _integrated_ flux in Jy/beam). The .app files files (which show the _apparent_ flux) are also created by default. Of course, if our observation was made with another instrument, we would not specify --Beam-Model LOFAR; instead, we would provide a .fits file which gives the value of the primary beam attenuation as a function of distance from pointing centre. This is beyond the scope of this tutorial, however, but know that it is supported by DDFacet.\n",
    "\n",
    "Note that our run also created a file called image.parset, which contains all the options used for the creation of that image. This is very convenient - it not only allows you to easily replicate a given run (changing only the salient parameters in the command line) but also serves as a \"history\" of what you actually did, and is therefore a good place to check for unexpected parameters if your images have unexpected problems. Shown below are the parameters which are most probably important to you, along with a short commentary giving tips for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[Data]\n",
    "MS = mydataset.MS                          # name of your ms. No default value.\n",
    "ColName = CORRECTED_DATA                   # name of column to image\n",
    "ChunkHours = 0.0                           # in case of memory error, set to 4, 2, 1...until it works\n",
    "\n",
    "[Selection]\n",
    "FlagAnts =                                 # regular expression or strings for ants to flag\n",
    "UVRangeKm = [0, 2000]                      # self evident\n",
    "\n",
    "[Output]\n",
    "Mode = Clean                               # can also be Dirty or PSF for different test cases\n",
    "Name = image                               # name of image you're making\n",
    "RestoringBeam = None                       # I would advise manually setting this to 4 times cell size\n",
    "Cubes =                                    # if you want freq. cubes. See -h\n",
    "Images = DdPAMRIikz                        # see DDF.py -h | grep Images \n",
    "\n",
    "[Image]\n",
    "NPix = 5000                                # number of pixels a side\n",
    "Cell = 5.0                                 # pixel size in arcsec\n",
    "PhaseCenterRADEC = align                   # where to put centre of image. If align, same as MS phase centre\n",
    "\n",
    "[Weight]\n",
    "ColName = WEIGHT_SPECTRUM                  # weight column to use\n",
    "Mode = Briggs                              # self evident but maybe you want to use uniform or something\n",
    "Robust = 0.0                               # see above\n",
    "\n",
    "[RIME]\n",
    "DecorrMode =                               # set to FT for wide fields of view. it's RIME magic.\n",
    "\n",
    "[Parallel]\n",
    "NCPU = 4                                   # number of CPUs used for parallelisation\n",
    "\n",
    "[Cache]\n",
    "Reset = True                               # good practice to set this to True when not changing image name.\n",
    "\n",
    "[Beam]\n",
    "Model = LOFAR                              # can be LOFAR or None. If you image eg GMRT/VLA data, provide fits beam file instead.\n",
    "LOFARBeamMode = AE                         # can be A or AE\n",
    "\n",
    "[Freq]\n",
    "NBand = 1                                  # use this when you want to make cubes: get as many freq. slices as NBand\n",
    "\n",
    "# after that, it's just CLEAN options.\n",
    "\n",
    "[Deconv]\n",
    "Mode = HMP \n",
    "MaxMajorIter = 20 \n",
    "MaxMinorIter = 20000 \n",
    "AllowNegative = True \n",
    "Gain = 0.1 \n",
    "FluxThreshold = 0.0 \n",
    "CycleFactor = 0.0 \n",
    "RMSFactor = 0.0 \n",
    "PeakFactor = 0.15 \n",
    "PrevPeakFactor = 0.0 \n",
    "NumRMSSamples = 10000 \n",
    "ApproximatePSF = 0 \n",
    "PSFBox = auto \n",
    "\n",
    "[Mask]\n",
    "External = None \n",
    "Auto = False \n",
    "SigTh = 10 \n",
    "FluxImageType = ModelConv \n",
    "\n",
    "\n",
    "[HMP]\n",
    "Alpha = [-1.0, 1.0, 11] \n",
    "Scales = [0] \n",
    "Ratios = [''] \n",
    "NTheta = 6 \n",
    "SolverMode = PI \n",
    "AllowResidIncrease = 0.1 \n",
    "MajorStallThreshold = 0.8 \n",
    "Taper = 0 \n",
    "Support = 0 \n",
    "PeakWeightImage = None \n",
    "Kappa = 0.0 \n",
    "OuterSpaceTh = 2.0 \n",
    "FractionRandomPeak = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's use the parset to redo our run, changing only the RIME-DecorrMode option to FT since we are making a widefield image. The first image should be just garbage with a stupidly bright source in the middle of the field and nothing around it, too. It seems that HMP is broken in this release of DDF for some reason, so let's use another deconvolution algorithm: SSD. It requires a mask to function, so we'll also set the automasking option to True, and it converges much faster than standard CLEAN, so we'll set the number of max major iterations to 5. While we're at it, we'll add a few more facets to make the intrinsic flux restored image smoother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDF.py image.parset --RIME-DecorrMode FT --Mask-Auto True --Facets-NFacets 20 --Deconv-Mode SSD --Output-RestoringBeam 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it should have crashed, and for quite a good reason: we asked DDF to load a parset, and then overwrite it in the same run. This is a terrible idea, and so DDF doesn't allow it. Let's change the name of our output image too, then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDF.py image.parset --RIME-DecorrMode FT --Mask-Auto True --Facets-NFacets 20 --Deconv-Mode SSD --Output-RestoringBeam 20 --Output-Name image.ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the structure of the command line options here, and compare it to the structure of the help message and the parset. For any given option, you give two dashes, then the main category (Beam, Output, Weight, Data, ...) comes first, then a dash, then the subcategory. The parset is organised in categories with a list of subcategories and associated values, as is the help. Capitalisation can be a bit haphazard, but that minor issue aside it is very easy to build your command line from the help and run with a parset so as to have either very long but complete command lines (best practice to avoid stupid mistakes, or at least maximise your chances of spotting them) or very short command lines (convenient but potentially a trap).\n",
    "\n",
    "Regardless, we have our first image. We could change some more parameters, such as the number of pixels (--Image-NPix) or the size of a pixel in arcsec (--Image-Cell); since our image doesn't seem to have bright sources whose sidelobes pollute our field, we can leave it at that for now. Let us now move on to making a catalog from our image, which we will then use to calibrate our image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **V. DD Self-cal - Second step: making a model from our image**\n",
    "\n",
    "This is a very quick step: there are many ways to go about it. Here, we will use a tool internal to the Wirtinger pack, called MakeModel. Know, however, that the same thing can be done using pybdsf and exporting the catalog as a \"bbs\"-format \"gaul\" - \"bbs\" being the name of a calibration solver used in prefactor, and gaul meaning GAUssian List. The advantage of using pybdsf is the power of the blob finder and the ability to group components into sources (can significantly speed up calibration time, especially for large extended emission). The drawback is that you don't use the CLEAN components directly; this can be a blessing and a curse.\n",
    "\n",
    "[see how long it takes to cal with makemodel on the node...if too long use pybdsf]\n",
    "Here are the commands to make a model from the clean components of a restored image and from a pybdsf catalog, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MakeModel.py --BaseImageName image.ft --NCluster 1\n",
    "MakeModel.py --SkyModel bbscatalog.txt --NCluster 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the --NCluster command sets the number of directions that killMS will try to find solutions for. If you use 1, then you will be performing direction-independent calibration, where the whole sky is treated as being in 1 direction; if you use e.g. --NCluster 40 you will have 40 directions to solve for in your primary beam. A good choice for the number of directions basically depends on your field, and the choice of directions is more art than science. In general, the LOFAR DR2 pipeline sets --NCluster to 40, as the fields tend to be relatively rich.\n",
    "\n",
    "Here, if we try the first method, we will get an error: one of the DDF dataproducts expected by MakeModel isn't part of the default output. We could either make the image again, adding \"--Output-Also all\". Instead, what we will do here is use pybdsf to create a bbs-style catalog file. Launch pybdsf from the command line by typing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pybdsf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now in the interactive pybdsf environment. We'll start by processing the image with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp process_image\n",
    "filename=\"image.ft.int.restored.fits\"\n",
    "go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are 4 tasks which pybdsf can perform. These are listed at launch, but you can find their names at any time by typing \"inp \" and tab-completing. Having processed the image, let's quickly look at what pybdsf picked up and the residuals. This is a good habit to develop, as sometimes spurious emission can be picked up, which will cause your calibration to bias you away from the true sky. Here, we simply type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp show_fit\n",
    "go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, typically, matplotlib is disabled. Never mind, then: let's just proceed to the catalog export. This is done with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp write_catalog\n",
    "outfile=\"image.ft.skymodel\"\n",
    "catalog_type=\"gaul\"\n",
    "format=\"bbs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above merely puts the sky model in a standard LOFAR format. The details aren't really important, but it can be helpful to know that \"gaul\" stands for \"GAUssian List\" - hence this format is a long list of gaussians with various properties - and \"bbs\", which stands for \"BlackBoard Selfcal\", is the predecessor to NDPPP, which most of you will know as a component of the prefactor pipeline. KillMS can read catalogs in the bbs format.\n",
    "\n",
    "Now that we have our model, we can run it through MakeModel. This is done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MakeModel.py --SkyModel image.ft.skymodel --NCluster 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of which method was used to generate the model, MakeModel will create a few components: a .reg file, which has the tesselation of the sky, and a .npy file, which has the sources associated to a given direction. Both are useful: the .reg file especially can give you useful information after the fact on whether part of the image being noisy is due to calibration failing for that facet, low signal to noise, etc. However, as far as killMS is concerned, all the salient info is encoded in the .npy file.\n",
    "\n",
    "Note that MakeModel is the piece of software that creates the facets used for direction-dependent calibration. Because we set NCluster to 1, there is only one facet here: the program tells you both the number of sources in the field and the number of directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **VI. DI Self-cal - Third step: calibrating our data from our model**\n",
    "\n",
    "Having made our model, we'll now proceed to calibration. First, a quick reminder of what calibration is, as opposed to imaging.\n",
    "\n",
    "Imaging consists of taking a set of visibilities, mapping them onto the image-plane using a Fast Fourier Transform (thus creating a so-called \"dirty image\"), and then using some kind of deconvolution algorithm to create a \"restored\" image, in which the impact of the PSF (which is huge for interferometric arrays, as they tend to sample the uv-plane very sparsely) is mitigated as much as possible.\n",
    "\n",
    "Calibration is usually a bit more difficult to wrap one's head around. To understand it, we must go back to the notion of the visibility and what it is an antenna measures. An antenna measures a voltage proportional to the strength of the incoming waves in the electromagnetic field (i.e. light) within its detection band. It is this voltage which is measured by single-dish antennas. A visibility is the correlation between the voltages recorded by two antennas at a given time.\n",
    "\n",
    "Now, the voltage measured by an antenna is assumed to be proportional to the radio signal sent by astronomical source. The proportionality factor between sky signal and antenna voltage is called the _gain_, and interferometry relies on the hypothesis that visibility gains can be decoupled into antenna gains. Calibration then consists of solving for the gain of all your antennas at all times and frequencies.\n",
    "\n",
    "KillMS, then, is the software used in the Wirtinger pack to solve for antenna gains. These can be direction-independent or direction-dependent - that is set by the number given in the NCluster parameter in MakeModel. Let's, once again, look at the options in killMS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "killMS.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there's a fair number of parameters, but less than with DDFacet. Unfortunately, there's also less documentation. The main things to know are the data selection options, notably that dt is in units of minutes. Also, if you use the Decorrelation-Mode FT in DDF, you want to use the equivalent here; similarly if you use the beam model in your imaging, you want to use it here also. Therefore, this is the calibration command we want to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "killMS.py --MSName test.ms --SkyModel image.ft.skymodel.npy --SolverType CohJones --dt 30 --NChanSols 2 --Decorrelation FT --NPCU 12--InCol CORRECTED_DATA --BeamModel LOFAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which will give us a new set of gains to use. Now we simply have to let DDFacet know which set of gains to read in when mapping the visibilities to the sky, and we're done with one round of selfcal! Note that, depending on your choice of calibration parameters, it is possible that your final DD-calibrated image may actually have regions (facets or groups of facets) which are worse than the original. In truly abhorrent cases, your entire image could be worse! When this happens, it usually means that there's a problem with your calibration. Check that you're using the correct sky model, that you're not making mistakes with the beam (use it in both calibration & imaging the same way, or don't use it at all), that your runs didn't crash or the computer goblins didn't mess with your data...\n",
    "\n",
    "and failing that, if you still have regions which give poor result, it's likely that your calibration parameters are such that you don't actually have sufficient signal to noise to calibrate those regions. From there you can either change your number of facets (either through a lower NCluster or handcrafted facets), or you can increase your dt and dnu for calibration solutions. Both approaches can work, and it will all depend on your personal requirements. Assuming that our calibration run finished successfully, let's take a look at what it did in our image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **VII. DI Self-cal - Fourth step: Imaging our newly-selfcal'd data**\n",
    "\n",
    "The only trick here, since we have calibrated using killMS, is to add a single parameter to our image command:\n",
    "\n",
    "--DDESolutions-DDSols test.ms/killMS.CohJones.sols.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDF.py --Data-MS test.ms --Parallel-NCPU 60 --Beam-Model LOFAR  --Output-Name image.ft.ssd.30min.sc --Deconv-Mode SSD --Deconv-MaxMajorIter 5 --Mask-Auto True  --Cache-Reset 1 --Output-Also \"all\" --DDESolutions-DDSols test.ms/killMS.CohJones.sols.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having added this parameter, let's take a quick look at the other DDESolutions parameters, and discuss which are the ones you probably want to know about:\n",
    "\n",
    "blablabla\n",
    "\n",
    "Now, let's take a look at the final image. blabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
