{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author: Etienne Bonnassieux\n",
    "## Comments, questions? Contact etienne.bonnassieux@unibo.it\n",
    "\n",
    "# **I. Prerequisites**\n",
    "\n",
    "In this jupyter notebook, we will learn how to perform a round of self-calibration using killMS & DDFacet on a LOFAR dataset. All the required software will be pre-installed on the singularity you'll be using for the school, and the data you will work on will be provided alongside it. If you want to do this later on your own machine and in your own time, you must first have the following installed (non-exhaustive list, I probably forgot at least one thing):\n",
    "\n",
    "- the KERN suite http://kernsuite.info/installation/\n",
    "- ddfacet - sudo apt-get install ddfacet; pip install ephem\n",
    "- dysco (if using a LOFAR dataset) - sudo apt-get install dysco\n",
    "- killMS, which you can acquire through saopicc: https://github.com/saopicc/killMS\n",
    "\n",
    "taking care to follow the instructions in individual gits where appropriate. Note that all the installs recommended here only work after installing KERN. If you don't want to use KERN, that is fine! Just be prepared for a good week of tears and sorrow. Note that KERN also comes with a lot of other useful radio-astro tools; I personally recommend tigger as an image viewer (takes a while to get used to but has some very useful functions) and pybdsf if you wish to get deeper than this tutorial in interferometric imaging.\n",
    "\n",
    "Note that you can also get the Wirtinger suite directly from the public release repositories:\n",
    "\n",
    "* https://github.com/saopicc/DDFacet\n",
    "* https://github.com/saopicc/killMS\n",
    "\n",
    "and follow the installation instructions there. There are usually a host of problems with that sort of installation, as with all radio-astronomy software. KERN allows you to avoid them, to an extent. Note that you can find the best \"documentation\" for DDFacet (and similarly for killMS, to a much lesser degree) in their respective parset.cfg file. They can be found here:\n",
    "\n",
    "https://github.com/saopicc/DDFacet/blob/master/DDFacet/Parset/DefaultParset.cfg\n",
    "https://github.com/saopicc/killMS/blob/master/Parset/DefaultParset.cfg\n",
    "\n",
    "\n",
    "Finally, should you want a slightly more convenient install than the above, Martin Hardcastle's DDF-pipeline installs them for you:\n",
    "\n",
    "* https://github.com/mhardcastle/ddf-pipeline\n",
    "\n",
    "For the purposes of the LOFAR school, the necessary software was installed as part of a local singularity by our talented and handsome colleagues in IT; the information above is therefore included for those users who wish to use this software to reduce data on machines they are responsible for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **II. Launching the singularity**\n",
    "\n",
    "The first thing we'll need to do is to set up the singularity properly so as to be able to carry out this tutorial session. The way to do this is specific to the Bologna cluster for now, as far as I'm aware, but efforts to standardise this approach across all italian LOFAR computing infrastructures are ongoing. Enter the following commands in bash. If they are in separate cells, they can be entered in clusters but not all at once (i.e. enter all the commands for one cell first, then all commands for the next, and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lofar-setup  190516.17pub\n",
    ". /irasoft/common/irainit/irainit_setup-alias.sh\n",
    "singularity-setup\n",
    "lofar-singularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source /opt/lofar-cfg.sh\n",
    "source /opt/lofar-init.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **II. Introduction**\n",
    "\n",
    "\n",
    "The aim of this tutorial is to calibrate & image a small LOFAR dataset using the so-called \"Wirtinger Pack\": DDFacet and killMS. Once you have finished this tutorial, you should have some basic understanding of how to run these software packages to reduce & image your own datasets. Here is the order of operations we will follow:\n",
    "\n",
    "- Inspecting the dataset\n",
    "- Making a first image using DDFacet\n",
    "- Making a model from this image\n",
    "- Perform a direction-independent calibration run on our data with our model\n",
    "- Imaging the newly-calibrated data\n",
    "- Do the same with more than 1 direction\n",
    "\n",
    "These are two separate self-calibration loops: one is direction-independent, and one is direction-dependent. We'll begin by creating a DATA folder, putting one of our pointing's dataset there, and loading the singlarity. Type the following commands in your command terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir DATA\n",
    "cp /iranet/home2/lofar/botteon/LOFAR_SCHOOL_DR1/GSM_CAL_L232875_ABN_174.tar.gz DATA\n",
    "cd DATA\n",
    "tar -xvf GSM_CAL_L232875_ABN_174.tar.gz\n",
    "rm *MS/*lock\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have thus created a DATA directory where we've moved our downloaded dataset, untarred it, removed the lock, and then we returned to our working directory. It can be inspected with tools like msoverview, casaviewer, or the pyrap packages; these are generally dependent on the casacore software. Let's inspect the dataset with pyrap to see what columns we have in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrap.tables import table\n",
    "t=table(\"prefactor/results/L232875_SB070_uv.dppp.pre-cal_124B2FCD4t_134MHz.pre-cal.ms/\")\n",
    "t.colnames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have a number of columns in this dataset. In particular, we have \"DATA\" and \"CORRECTED_DATA\". This tells us that the data was calibrated already; in this case, it was calibrated with prefactor in amplitude and phase. Let's image this calibrated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IV. DI Self-cal - First step: imaging prefactored data with DDFacet**\n",
    "\n",
    "\n",
    "Here, we will explore using DDFacet to image some interferometric data contained in a Measurement Set, the standard dataset for LOFAR. Let's start by taking a little look at the options available. Run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDF.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there's quite a few!...in fact, the vast majority of them can be safely ignored. For starters, let's make a small image with our dataset. Since we're running this on LOFAR surveys data, for which the software was developed, we can safely run it with mostly default options. We will use three non-default options here: first, we specify the primary beam of our observation, as this will allow us to get images in apparenty and integrated flux (--Beam-Model LOFAR). We will also specify the number of CPUs to use explicitly, to avoid taxing the machine too much (--Parallel-NCPU 4). Finally, since there is no default for the dataset to be imaged, we specify it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " DDF.py --Data-MS prefactor/results/L232875_SB070_uv.dppp.pre-cal_124B2FCD4t_134MHz.pre-cal.ms/ --Parallel-NCPU 4 --Beam-Model LOFAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all went well, you should now have a restored image of our prefactored data. Let's take a look at a few of the images created by our run here: by default, these will all have the prefix \"image\" and be in the current directory. We can inspect the fits files with ds9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds9 image*fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should open a lot of different images. The ones you are most likely to want to inspect are:\n",
    "\n",
    "- image.int.restored.fits (if you applied the LOFAR beam)\n",
    "- image.dirty.fits        (to see if anything was catastrophically wrong with the data)\n",
    "- image.int.residual.fits (to see what your CLEAN components are)\n",
    "\n",
    "in this example, because we specified a primary beam model, it was automatically applied to create the .int files (which show the _integrated_ flux in Jy/beam). The .app files files (which show the _apparent_ flux) are also created by default. Of course, if our observation was made with another instrument, we would not specify --Beam-Model LOFAR; instead, we would provide a .fits file which gives the value of the primary beam attenuation as a function of distance from pointing centre. This is beyond the scope of this tutorial, however, but know that it is supported by DDFacet\n",
    "\n",
    "Note that our run also created a file called image.parset, which contains all the options used for the creation of that image. This is very convenient - it not only allows you to easily replicate a given run (changing only the salient parameters in the command line) but also serves as a \"history\" of what you actually did, and is therefore a good place to check for unexpected parameters if your images have unexpected problems. Shown below are the parameters which are most probably important to you, along with a short commentary giving tips for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[Data]\n",
    "MS = mydataset.MS                          # name of your ms. No default value.\n",
    "ColName = CORRECTED_DATA                   # name of column to image\n",
    "ChunkHours = 0.0                           # in case of memory error, set to 4, 2, 1...until it works\n",
    "\n",
    "[Selection]\n",
    "FlagAnts =                                 # regular expression or strings for ants to flag\n",
    "UVRangeKm = [0, 2000]                      # self evident\n",
    "\n",
    "[Output]\n",
    "Mode = Clean                               # can also be Dirty or PSF for different test cases\n",
    "Name = image                               # name of image you're making\n",
    "RestoringBeam = None                       # I would advise manually setting this to 4 times cell size\n",
    "Cubes =                                    # if you want freq. cubes. See -h\n",
    "Images = DdPAMRIikz                        # see DDF.py -h | grep Images \n",
    "\n",
    "[Image]\n",
    "NPix = 5000                                # number of pixels a side\n",
    "Cell = 5.0                                 # pixel size in arcsec\n",
    "PhaseCenterRADEC = align                   # where to put centre of image. If align, same as MS phase centre\n",
    "\n",
    "[Weight]\n",
    "ColName = WEIGHT_SPECTRUM                  # weight column to use\n",
    "Mode = Briggs                              # self evident but maybe you want to use uniform or something\n",
    "Robust = 0.0                               # see above\n",
    "\n",
    "[RIME]\n",
    "DecorrMode =                               # set to FT for wide fields of view. it's RIME magic.\n",
    "\n",
    "[Parallel]\n",
    "NCPU = 4                                   # number of CPUs used for parallelisation\n",
    "\n",
    "[Cache]\n",
    "Reset = True                               # good practice to set this to True when not changing image name.\n",
    "\n",
    "[Beam]\n",
    "Model = LOFAR                              # can be LOFAR or None. If you image eg GMRT/VLA data, provide fits beam file instead.\n",
    "LOFARBeamMode = AE                         # can be A or AE\n",
    "\n",
    "[Freq]\n",
    "NBand = 1                                  # use this when you want to make cubes: get as many freq. slices as NBand\n",
    "\n",
    "# after that, it's just CLEAN options.\n",
    "\n",
    "[Deconv]\n",
    "Mode = HMP \n",
    "MaxMajorIter = 20 \n",
    "MaxMinorIter = 20000 \n",
    "AllowNegative = True \n",
    "Gain = 0.1 \n",
    "FluxThreshold = 0.0 \n",
    "CycleFactor = 0.0 \n",
    "RMSFactor = 0.0 \n",
    "PeakFactor = 0.15 \n",
    "PrevPeakFactor = 0.0 \n",
    "NumRMSSamples = 10000 \n",
    "ApproximatePSF = 0 \n",
    "PSFBox = auto \n",
    "\n",
    "[Mask]\n",
    "External = None \n",
    "Auto = False \n",
    "SigTh = 10 \n",
    "FluxImageType = ModelConv \n",
    "\n",
    "\n",
    "[HMP]\n",
    "Alpha = [-1.0, 1.0, 11] \n",
    "Scales = [0] \n",
    "Ratios = [''] \n",
    "NTheta = 6 \n",
    "SolverMode = PI \n",
    "AllowResidIncrease = 0.1 \n",
    "MajorStallThreshold = 0.8 \n",
    "Taper = 0 \n",
    "Support = 0 \n",
    "PeakWeightImage = None \n",
    "Kappa = 0.0 \n",
    "OuterSpaceTh = 2.0 \n",
    "FractionRandomPeak = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's use the parset to redo our run, changing only the RIME-DecorrMode option to FT since we are making a widefield image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDF.py image.parset --RIME-DecorrMode FT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it should have crashed, and for quite a good reason: we asked DDF to load a parset, and then overwrite it in the same run. This is a terrible idea, and so DDF doesn't allow it. Let's change the name of our output image too, then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDF.py image.parset --RIME-DecorrMode FT --Output-Name image.ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the structure of the command line options here, and compare it to the structure of the help message and the parset. For any given option, you give two dashes, then the main category (Beam, Output, Weight, Data, ...) comes first, then a dash, then the subcategory. The parset is organised in categories with a list of subcategories and associated values, as is the help. Capitalisation can be a bit haphazard, but that minor issue aside it is very easy to build your command line from the help and run with a parset so as to have either very long but complete command lines (best practice to avoid stupid mistakes, or at least maximise your chances of spotting them) or very short command lines (convenient but potentially a trap).\n",
    "\n",
    "Regardless, we have our first image. We could change some more parameters, such as the number of pixels (--Image-NPix) or the size of a pixel in arcsec (--Image-Cell); since our image doesn't seem to have bright sources whose sidelobes pollute our field, we can leave it at that for now. Let us now move on to making a catalog from our image, which we will then use to calibrate our image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **V. DI Self-cal - Second step: making a model from our image**\n",
    "\n",
    "This is a very quick step: there are many ways to go about it. Here, we will use a tool internal to the Wirtinger pack, called MakeModel. Know, however, that the same thing can be done using pybdsf and exporting the catalog as a \"bbs\"-format \"gaul\" - \"bbs\" being the name of a calibration solver used in prefactor, and gaul meaning GAUssian List. The advantage of using pybdsf is the power of the blob finder and the ability to group components into sources (can significantly speed up calibration time, especially for large extended emission). The drawback is that you don't use the CLEAN components directly; this can be a blessing and a curse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **VI. DI Self-cal - Third step: calibrating our data from our model**\n",
    "\n",
    "bla bla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **VII. DI Self-cal - Fourth step: Imaging our newly-selfcal'd data**\n",
    "\n",
    "bla bla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **VIII. DD Self-cal - First step: imaging DI-selfcal data with DDFacet**\n",
    "\n",
    "bla bla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IX. DD Self-cal - Second step: making a model from our new image**\n",
    "\n",
    "bla bla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **X. DD Self-cal - Third step: calibrating our data from our model**\n",
    "\n",
    "bla bla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **VII. DD Self-cal - Fourth step: Imaging our newly-selfcal'd data**\n",
    "\n",
    "bla bla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
